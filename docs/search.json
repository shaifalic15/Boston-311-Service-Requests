[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Boston 311 Service Requests",
    "section": "",
    "text": "1 Introduction\nTopic: Boston 311 Service Requests\nWe chose the Boston 311 Service Requests topic because it offers a unique window into how a city addresses everyday challenges faced by its residents. From potholes to noise complaints, this data captures the pulse of Boston’s public services and the issues that matter most to its communities. By analyzing Boston 311 Service Requests dataset, we aim to uncover patterns in the types of service requests, their geographical distribution, and how efficiently they are resolved. For readers unfamiliar with 311 systems, they serve as a vital link between residents and city departments, enabling people to report non-emergency issues and request services. Exploring this data not only highlights the responsiveness of urban governance but also sheds light on potential areas for improvement in delivering public services effectively."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nFor this project, we are using the latest Boston 311 Service Requests dataset of year 2024, which is publicly available on the Boston.gov open data portal. This dataset is maintained by the City of Boston and contains detailed records of non-emergency service requests submitted by residents through the city’s 311 system. The data includes fields such as request types, submission dates, locations, statuses, and resolution times, offering a comprehensive view of the city’s service management. The dataset is updated regularly, ensuring it reflects current trends, although the exact frequency of updates is not explicitly specified It is provided in a tabular format, accessible as a CSV file, and includes both categorical and geographic data, such as latitude, longitude, and neighborhood identifiers. One potential challenge is the presence of missing or inconsistent data, particularly in fields like “closure reason” or “submitted photo,” which will require preprocessing. I plan to import the dataset into RStudio using the read.csv function for cleaning and analysis. The dataset can be accessed from its original source at Boston 311 Service Requests with the year 2024.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\n# load packages and read in data\n\nlibrary(readr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ tibble  3.2.1     ✔ stringr 1.5.0\n✔ tidyr   1.3.1     ✔ forcats 0.5.2\n✔ purrr   1.0.1     \n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf <- read_csv('/Users/andyyang/desktop/EDAV/boston311.csv')\n\n\nRows: 274415 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (20): on_time, case_status, closure_reason, case_title, subject, reason...\ndbl   (6): case_enquiry_id, fire_district, city_council_district, neighborho...\nlgl   (1): submitted_photo\ndttm  (3): open_dt, sla_target_dt, closed_dt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# pivot and calculate the number of missing values per variable\nmissing_percent <- df %>%\n  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %>%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Variable\",\n    values_to = \"MissingPercent\"\n  ) %>%\n  arrange(desc(MissingPercent))\n\n# create the bar plot\nggplot(missing_percent, aes(x = reorder(Variable, -MissingPercent), y = MissingPercent)) +\n  geom_bar(stat = \"identity\", fill = \"tomato\") +\n  theme_minimal() +\n  labs(\n    title = \"Percentage of Missing Values per Variable\",\n    x = \"Variables\",\n    y = \"Percentage of Missing Values (%)\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5),\n    axis.title.y = element_text(margin = margin(r = 10))\n  ) +\n  geom_text(aes(label = sprintf(\"%.1f%%\", MissingPercent)), \n            angle = 45,\n            hjust = -0.1, \n            vjust = -0.5, \n            size = 2.5) +\n  ylim(0, max(missing_percent$MissingPercent) * 1.1)\n\n\n\n\n\nBy graphing the percent of null values in each column, we can see that null values make up all of the “submitted_photo” column, and most of the “closed_photo” columns. Moreover, considering that these columns just contain links to photographs taken at the scene of the 311 request, these columns will not very important to our analysis.\nAdditionally, columns such as “closure_reason”, “closed_dt”, “location_zipcode”, and “sla_target_dt” are around 20% null values. This would only potentially impact analysis related to request response times, as we have other columns such as “location” and “latitude”/“longitude” to help us understand geographical trends, and columns such as “type”, “reason”, and “subject” to help us with request categorization.\nAll of the other columns either have 0 null values, or a very small percentage (< 1.5%) of null values, indicating that the dataset is generally complete and reliable for most analyses.\n\n\nCode\n# https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html\n# install.packages(\"naniar\")\nlibrary(naniar)\n\nnaniar::vis_miss(df,warn_large_data = FALSE) +\n  labs(\n    title = \"Missing Data Heatmap\",\n    subtitle = \"Visual Representation of Missing Values in a Subset of df\"\n  )\n\n\n\n\n\nThis plot provides a more detailed visualiation of the amount ane location of missing data, and also providing information on the overall percentage of missing values overall in the legend.\nSpecifically, we can infer that missingness is not exactly uniform, meaning that there may be clusters of rows with higher concentrations of missing data. For example, in the “closed_photo” column of the plot, we can identify a few clusters where there is more/less missing data. We also get a proportion on the total amount of missing values in our dataset–8.6%, indicating that while the overall proportion of missing data is relatively low."
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description\nFor this project, we are using the latest Boston 311 Service Requests dataset of year 2024, which is publicly available on the Boston.gov open data portal. This dataset is maintained by the City of Boston and contains detailed records of non-emergency service requests submitted by residents through the city’s 311 system. The data includes fields such as request types, submission dates, locations, statuses, and resolution times, offering a comprehensive view of the city’s service management. The dataset is updated regularly, ensuring it reflects current trends, although the exact frequency of updates is not explicitly specified. It is provided in a tabular format, accessible as a CSV file, and includes both categorical and geographic data, such as latitude, longitude, and neighborhood identifiers. One potential challenge is the presence of missing or inconsistent data, particularly in fields like “closure reason” or “submitted photo,” which will require preprocessing. We plan to import the dataset into RStudio using the read.csv function for cleaning and analysis. The dataset can be accessed from its original source at Boston 311 Service Requests with the year 2024. There are 274,415 total rows and 30 columns in this dataset."
  }
]